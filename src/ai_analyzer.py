#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ–‡ä»¶åˆ†æå™¨
ä½¿ç”¨AIåˆ†ææå–çš„gitå¯¹è±¡æ–‡ä»¶ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰ä»·å€¼å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import os
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from .config_manager import ConfigManager
import time
from collections import deque

class AIAnalyzer:
    def __init__(self, ai_config_key: str = "moonshot", custom_api_key: str = ""):
        self.config_manager = ConfigManager()
        self.ai_config_key = ai_config_key
        
        # è·å–AIé…ç½®
        self.config = self.config_manager.get_ai_config(ai_config_key)
        
        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰APIå¯†é’¥ï¼Œä½¿ç”¨å®ƒ
        if custom_api_key:
            self.config["api_key"] = custom_api_key
            
        # ç¡®ä¿APIå¯†é’¥ä¸ä¸ºç©º
        if not self.config["api_key"]:
            raise ValueError(f"AIé…ç½® '{ai_config_key}' çš„APIå¯†é’¥ä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡")
            
        self.client = OpenAI(
            api_key=self.config["api_key"],
            base_url=self.config["base_url"]
        )
        
        # è·å–åˆ†æé…ç½®
        analysis_config = self.config_manager.get_analysis_config()
        self.max_workers = analysis_config["max_workers"]
        self.max_content_length = analysis_config["max_content_length"]
        self.temperature = analysis_config["temperature"]
        
        # ========================  è¯·æ±‚é€Ÿç‡é™åˆ¶  ========================
        # ä»é…ç½®è¯»å– RPM é™åˆ¶
        self._rpm_limit = analysis_config.get("rpm_limit", 200)
        self._request_timestamps: deque = deque()  # å­˜å‚¨æœ€è¿‘ä¸€æ¬¡çš„è¯·æ±‚æ—¶é—´æˆ³
        # ============================================================
        
        # è·å–é¢„è§ˆé•¿åº¦é…ç½®
        self.file_analysis_preview_length = analysis_config["file_analysis_preview_length"]
        self.batch_grouping_preview_length = analysis_config["batch_grouping_preview_length"]
        self.iterative_similarity_preview_length = analysis_config["iterative_similarity_preview_length"]
        self.version_analysis_preview_length = analysis_config["version_analysis_preview_length"]
        
    def _enforce_rate_limit(self):
        """ç¡®ä¿è¯·æ±‚é€Ÿç‡ä¸è¶…è¿‡ rpm é™åˆ¶"""
        now = time.time()
        window = 60  # ç§’
        # æ¸…ç†è¿‡æœŸæ—¶é—´æˆ³
        while self._request_timestamps and now - self._request_timestamps[0] > window:
            self._request_timestamps.popleft()
        if len(self._request_timestamps) >= self._rpm_limit:
            # éœ€è¦ç­‰å¾…
            sleep_time = window - (now - self._request_timestamps[0]) + 0.05  # ç•™ä¸€ç‚¹ä½™é‡
            print(f"è¾¾åˆ°é€Ÿç‡ä¸Šé™({self._rpm_limit} RPM)ï¼Œæš‚åœ {sleep_time:.2f}s â€¦")
            time.sleep(sleep_time)
        # è®°å½•æœ¬æ¬¡è¯·æ±‚
        self._request_timestamps.append(time.time())

    def chat_complete(self, messages, model: Optional[str] = None,
                      temperature: Optional[float] = None, response_format=None, timeout: Optional[int] = None, stream: bool = False):
        """å°è£… openai chat.completions.createï¼Œç»Ÿä¸€é€Ÿç‡é™åˆ¶"""
        self._enforce_rate_limit()
        return self.client.chat.completions.create(
            model=model or self.config["model"],
            messages=messages,
            temperature=temperature if temperature is not None else self.temperature,
            response_format=response_format or {"type": "json_object"},
            timeout=timeout,
            stream=stream
        )
    
    def chat_complete_stream(self, messages, model: Optional[str] = None,
                           temperature: Optional[float] = None, response_format=None, timeout: Optional[int] = None, silent: bool = False):
        """æµå¼è°ƒç”¨AIï¼Œå®æ—¶æ˜¾ç¤ºè¾“å‡º"""
        self._enforce_rate_limit()
        
        if not silent:
            print("ğŸ¤– AIå¼€å§‹å“åº”...")
        start_time = time.time()
        
        try:
            stream = self.client.chat.completions.create(
                model=model or self.config["model"],
                messages=messages,
                temperature=temperature if temperature is not None else self.temperature,
                response_format=response_format or {"type": "json_object"},
                timeout=timeout,
                stream=True
            )
            
            full_response = ""
            chunk_count = 0
            
            for chunk in stream:
                chunk_count += 1
                if hasattr(chunk.choices[0], 'delta') and chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    if not silent:
                        print(content, end="", flush=True)
                    
                    # æ¯100ä¸ªchunkæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if not silent and chunk_count % 100 == 0:
                        elapsed = time.time() - start_time
                        print(f"\nâ±ï¸  å·²å¤„ç† {chunk_count} ä¸ªchunkï¼Œè€—æ—¶: {elapsed:.2f}s")
            
            elapsed = time.time() - start_time
            if not silent:
                print(f"\nâœ… AIå“åº”å®Œæˆï¼æ€»è€—æ—¶: {elapsed:.2f}sï¼Œå¤„ç†äº† {chunk_count} ä¸ªchunk")
                print(f"ğŸ“ å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
            
            return full_response
            
        except Exception as e:
            elapsed = time.time() - start_time
            if not silent:
                print(f"\nâŒ AIæµå¼è°ƒç”¨å¤±è´¥ï¼è€—æ—¶: {elapsed:.2f}s")
                print(f"é”™è¯¯ä¿¡æ¯: {e}")
            raise e
        
    def analyze_file_with_ai(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨AIåˆ†ææ–‡ä»¶å†…å®¹ï¼Œè·å–ç®€çŸ­åˆ†æã€å»ºè®®å‘½åå’Œæ–‡ä»¶ç±»å‹åˆ¤æ–­ã€‚
        """
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # é™åˆ¶æ–‡ä»¶å†…å®¹å¤§å°ï¼Œä½¿ç”¨æ–‡ä»¶åˆ†æé¢„è§ˆé•¿åº¦
            preview_length = self.file_analysis_preview_length
            if preview_length > 0 and len(content) > preview_length:
                content = content[:preview_length] + "\n... [å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"
            elif preview_length == -1:
                # æ— é™åˆ¶ï¼Œä½†ä»æœ‰æœ€å¤§é•¿åº¦ä¿æŠ¤
                if len(content) > self.max_content_length:
                    content = content[:self.max_content_length] + "\n... [å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"

            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªæ–‡ä»¶åˆ†æåŠ©æ‰‹ï¼Œä½ éœ€è¦åˆ†ææˆ‘æä¾›çš„æ–‡ä»¶å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æœ‰ä»·å€¼çš„æ–‡ä»¶ã€‚
            
            æœ‰ä»·å€¼çš„æ–‡ä»¶åŒ…æ‹¬ï¼š
            - ä»£ç æ–‡ä»¶ï¼ˆ.py, .js, .java, .cppç­‰ï¼‰
            - æ–‡æ¡£æ–‡ä»¶ï¼ˆ.md, .txt, .rstç­‰ï¼‰
            - é…ç½®æ–‡ä»¶ï¼ˆ.json, .yaml, .toml, .iniç­‰ï¼‰
            - å…¶ä»–äººç±»å¯è¯»çš„æ–‡ä»¶
            
            æ²¡æœ‰ä»·å€¼çš„æ–‡ä»¶åŒ…æ‹¬ï¼š
            - Git ls-treeè¾“å‡ºï¼ˆæ ¼å¼å¦‚ï¼š100644 blob hash filenameï¼‰
            - Git diffè¾“å‡º
            - Git logè¾“å‡º
            - çº¯äºŒè¿›åˆ¶æ–‡ä»¶å†…å®¹
            - ç³»ç»Ÿç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
            
            ç‰¹æ®Šæƒ…å†µï¼š
            - å¦‚æœæ–‡ä»¶æ•´ä½“æ˜¯æœ‰ä»·å€¼çš„ä»£ç æˆ–æ–‡æ¡£ï¼Œä½†åŒ…å«gitå†²çªæ ‡è®°ï¼ˆ<<<<<<< HEAD, =======, >>>>>>>ï¼‰ï¼Œä»ç„¶è®¤ä¸ºæ˜¯æœ‰ä»·å€¼çš„
            
            ç¤ºä¾‹ï¼š
            
            ç¤ºä¾‹1ï¼ˆæ— ä»·å€¼ - Git ls-treeè¾“å‡ºï¼‰ï¼š
            ```
            100644 blob dfe0770424b2a19faf507a501ebfc23be8f54e7b	.gitattributes
            100644 blob 08e8ba8d6afcd9cebf04888ec1aad55350f8b71d	.gitignore
            100644 blob c31561c8644dc81fb40ee784bf1e55ae0279e669	API_REFERENCE.md
            ```
            åˆ†æï¼šè¿™æ˜¯git ls-treeå‘½ä»¤çš„è¾“å‡ºï¼Œæ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨å’Œhashï¼Œä¸æ˜¯å®é™…æ–‡ä»¶å†…å®¹
            å»ºè®®ï¼šæ— ä»·å€¼
            
            ç¤ºä¾‹2ï¼ˆæœ‰ä»·å€¼ - åŒ…å«å†²çªæ ‡è®°çš„æ–‡æ¡£ï¼‰ï¼š
            ```
            # AIåˆ›æ„å·¥ä½œæµç³»ç»Ÿ
            ä¸€ä¸ªåŸºäº"æ©ç åˆ†è§£"ç†è®ºçš„è½»é‡çº§AIåˆ›ä½œå·¥ä½œæµæ¡†æ¶...
            <<<<<<< HEAD
            â”œâ”€â”€ models.ini            # æ¨¡å‹é…ç½®æ–‡ä»¶
            =======
            >>>>>>> 7e7ea78 (Refactor project structure...)
            ```
            åˆ†æï¼šè¿™æ˜¯æœ‰ä»·å€¼çš„READMEæ–‡æ¡£ï¼Œè™½ç„¶åŒ…å«gitå†²çªæ ‡è®°ï¼Œä½†ä¸»è¦å†…å®¹æ˜¯æ–‡æ¡£
            å»ºè®®ï¼šæœ‰ä»·å€¼
            
            ç¤ºä¾‹3ï¼ˆæœ‰ä»·å€¼ - ä»£ç æ–‡ä»¶ï¼‰ï¼š
            ```
            import os
            import json
            from openai import OpenAI
            
            def analyze_file_with_ai(file_path, client):
                # å‡½æ•°å®ç°...
            ```
            åˆ†æï¼šè¿™æ˜¯Pythonä»£ç æ–‡ä»¶ï¼ŒåŒ…å«å®é™…çš„ä»£ç é€»è¾‘
            å»ºè®®ï¼šæœ‰ä»·å€¼
            
            ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - 'name': å»ºè®®æ–‡ä»¶åï¼ŒåŒ…å«åç¼€
            - 'analysis': ç®€çŸ­åˆ†æï¼ˆ100å­—ä»¥å†…ï¼‰
            - 'valuable': å¸ƒå°”å€¼ï¼Œå½“æ–‡ä»¶ä¸ºæœ‰ä»·å€¼çš„æ–‡ä»¶æ—¶ä¸ºçœŸ
            - 'file_type': æ–‡ä»¶ç±»å‹ï¼ˆå¦‚ï¼špython, javascript, markdown, textç­‰ï¼‰
            - 'confidence': ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
            """
            
            user_prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶å†…å®¹ï¼š\n\n```\n{content}\n```"

            completion = self.chat_complete(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                stream=False
            )

            # ç±»å‹æ–­è¨€ï¼Œç¡®ä¿completionæ˜¯ChatCompletionå¯¹è±¡è€Œä¸æ˜¯Streamå¯¹è±¡
            response_content = completion.choices[0].message.content  # type: ignore
            if(response_content == None):
                return None
            return json.loads(response_content)

        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def get_file_extension(self, file_type: str, suggested_name: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹å’Œå»ºè®®åç§°è·å–æ–‡ä»¶æ‰©å±•å"""
        # å¦‚æœå»ºè®®åç§°å·²ç»åŒ…å«æ‰©å±•åï¼Œç›´æ¥ä½¿ç”¨
        if '.' in suggested_name:
            return suggested_name.split('.')[-1]
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è·å–æ‰©å±•å
        return self.config_manager.get_file_extension(file_type.lower())
    
    def generate_new_filename(self, original_filename: str, ai_response: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–°çš„æ–‡ä»¶å"""
        suggested_name = ai_response.get('name', 'untitled')
        file_type = ai_response.get('file_type', 'text')
        
        # è·å–åŸå§‹å“ˆå¸Œå‰ç¼€ï¼ˆå‰8ä½ï¼‰
        hash_prefix = original_filename.split('.')[0][:8]
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        extension = self.get_file_extension(file_type, suggested_name)
        
        # ç”Ÿæˆæ–°æ–‡ä»¶å
        if '.' in suggested_name:
            # å¦‚æœå»ºè®®åç§°åŒ…å«æ‰©å±•åï¼Œä½¿ç”¨å®ƒ
            base_name = suggested_name.split('.')[0]
            return f"{base_name}_{hash_prefix}.{extension}"
        else:
            return f"{suggested_name}_{hash_prefix}.{extension}"
    
    def analyze_directory(self, input_dir: str, output_dir: str) -> Dict[str, Any]:
        """åˆ†æç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_path}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"å¼€å§‹AIåˆ†æ...")
        print(f"è¾“å…¥ç›®å½•: {input_path}")
        print(f"è¾“å‡ºç›®å½•: {output_path}")
        print(f"AIæ¨¡å‹: {self.config['name']} ({self.config['model']})")
        print()
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        arc_files = [f for f in input_path.iterdir() if f.is_file() and f.suffix == '.txt']
        
        if not arc_files:
            print("è­¦å‘Š: è¾“å…¥ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.txtæ–‡ä»¶")
            return {"analyzed_count": 0, "saved_count": 0, "results": []}
        
        print(f"æ‰¾åˆ° {len(arc_files)} ä¸ªæ–‡ä»¶éœ€è¦åˆ†æ")
        
        results = []
        saved_count = 0
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘è¯·æ±‚
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {executor.submit(self.analyze_file_with_ai, file_path): file_path for file_path in arc_files}
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                original_filename = file_path.name
                
                try:
                    ai_response = future.result()
                    
                    if ai_response:
                        # ç”Ÿæˆæ–°æ–‡ä»¶å
                        new_filename = self.generate_new_filename(original_filename, ai_response)
                        
                        # è®°å½•åˆ†æç»“æœ
                        result_entry = {
                            "original_filename": original_filename,
                            "original_hash": original_filename.split('.')[0],
                            "ai_analysis": ai_response,
                            "suggested_filename": new_filename,
                            "saved": False,
                            "saved_path": None
                        }
                        
                        # å¦‚æœæ–‡ä»¶æœ‰ä»·å€¼ï¼Œä¿å­˜åˆ°è¾“å‡ºç›®å½•
                        if ai_response.get('valuable', False):
                            new_file_path = output_path / new_filename
                            
                            try:
                                shutil.copy2(file_path, new_file_path)
                                saved_count += 1
                                result_entry["saved"] = True
                                result_entry["saved_path"] = str(new_file_path)
                                print(f"âœ“ ä¿å­˜: {original_filename} -> {new_filename}")
                            except Exception as e:
                                print(f"âœ— ä¿å­˜å¤±è´¥: {original_filename} -> {new_filename}: {e}")
                        else:
                            print(f"- è·³è¿‡: {original_filename} (æ— ä»·å€¼)")
                        
                        results.append(result_entry)
                    else:
                        print(f"âœ— åˆ†æå¤±è´¥: {original_filename}")
                        results.append({
                            "original_filename": original_filename,
                            "original_hash": original_filename.split('.')[0],
                            "ai_analysis": None,
                            "suggested_filename": None,
                            "saved": False,
                            "saved_path": None,
                            "error": "AIåˆ†æå¤±è´¥"
                        })
                        
                except Exception as exc:
                    print(f"âœ— å¤„ç†å¼‚å¸¸: {original_filename}: {exc}")
                    results.append({
                        "original_filename": original_filename,
                        "original_hash": original_filename.split('.')[0],
                        "ai_analysis": None,
                        "suggested_filename": None,
                        "saved": False,
                        "saved_path": None,
                        "error": str(exc)
                    })
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis_summary = {
            "analysis_info": {
                "ai_model": self.config["name"],
                "model_name": self.config["model"],
                "input_directory": str(input_path),
                "output_directory": str(output_path),
                "total_files": len(arc_files),
                "analyzed_count": len(results),
                "saved_count": saved_count,
                "skipped_count": len(arc_files) - saved_count
            },
            "results": results
        }
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_file = output_path / "analysis_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nåˆ†æå®Œæˆ!")
        print(f"æ€»æ–‡ä»¶æ•°: {len(arc_files)}")
        print(f"åˆ†ææˆåŠŸ: {len(results)}")
        print(f"ä¿å­˜æ–‡ä»¶: {saved_count}")
        print(f"è·³è¿‡æ–‡ä»¶: {len(arc_files) - saved_count}")
        print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return analysis_summary

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    input_dir = "extracted_objects"
    output_dir = "analyzed_files"
    
    # åˆ›å»ºAIåˆ†æå™¨
    analyzer = AIAnalyzer(ai_config_key="moonshot")
    
    # æ‰§è¡Œåˆ†æ
    try:
        results = analyzer.analyze_directory(input_dir, output_dir)
        print(f"åˆ†æå®Œæˆï¼Œç»“æœ: {results}")
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}") 